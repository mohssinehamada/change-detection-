{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qidpKOu_99GQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image, ImageOps, ImageFilter\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class Normalize(object):\n",
        "    \"\"\"Normalize a tensor image with mean and standard deviation.\n",
        "    Args:\n",
        "        mean (tuple): means for each channel.\n",
        "        std (tuple): standard deviations for each channel.\n",
        "    \"\"\"\n",
        "    def __init__(self, mean=(0., 0., 0.), std=(1., 1., 1.)):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        img = sample['image']\n",
        "        mask = sample['label']\n",
        "        img = np.array(img).astype(np.float32)\n",
        "        mask = np.array(mask).astype(np.float32)\n",
        "        img /= 255.0\n",
        "        img -= self.mean\n",
        "        img /= self.std\n",
        "\n",
        "        return {'image': img,\n",
        "                'label': mask}\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        img1 = sample['image'][0]\n",
        "        img2 = sample['image'][1]\n",
        "        mask = sample['label']\n",
        "        img1 = np.array(img1).astype(np.float32).transpose((2, 0, 1))\n",
        "        img2 = np.array(img2).astype(np.float32).transpose((2, 0, 1))\n",
        "        mask = np.array(mask).astype(np.float32) / 255.0\n",
        "\n",
        "        img1 = torch.from_numpy(img1).float()\n",
        "        img2 = torch.from_numpy(img2).float()\n",
        "        mask = torch.from_numpy(mask).float()\n",
        "\n",
        "        return {'image': (img1, img2),\n",
        "                'label': mask}\n",
        "\n",
        "\n",
        "class RandomHorizontalFlip(object):\n",
        "    def __call__(self, sample):\n",
        "        img1 = sample['image'][0]\n",
        "        img2 = sample['image'][1]\n",
        "        mask = sample['label']\n",
        "        if random.random() < 0.5:\n",
        "            img1 = img1.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "            img2 = img2.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "        return {'image': (img1, img2),\n",
        "                'label': mask}\n",
        "\n",
        "class RandomVerticalFlip(object):\n",
        "    def __call__(self, sample):\n",
        "        img1 = sample['image'][0]\n",
        "        img2 = sample['image'][1]\n",
        "        mask = sample['label']\n",
        "        if random.random() < 0.5:\n",
        "            img1 = img1.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "            img2 = img2.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "            mask = mask.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "\n",
        "        return {'image': (img1, img2),\n",
        "                'label': mask}\n",
        "\n",
        "class RandomFixRotate(object):\n",
        "    def __init__(self):\n",
        "        self.degree = [Image.ROTATE_90, Image.ROTATE_180, Image.ROTATE_270]\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        img1 = sample['image'][0]\n",
        "        img2 = sample['image'][1]\n",
        "        mask = sample['label']\n",
        "        if random.random() < 0.75:\n",
        "            rotate_degree = random.choice(self.degree)\n",
        "            img1 = img1.transpose(rotate_degree)\n",
        "            img2 = img2.transpose(rotate_degree)\n",
        "            mask = mask.transpose(rotate_degree)\n",
        "\n",
        "        return {'image': (img1, img2),\n",
        "                'label': mask}\n",
        "\n",
        "\n",
        "class RandomRotate(object):\n",
        "    def __init__(self, degree):\n",
        "        self.degree = degree\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        img1 = sample['image'][0]\n",
        "        img2 = sample['image'][1]\n",
        "        mask = sample['label']\n",
        "        rotate_degree = random.uniform(-1*self.degree, self.degree)\n",
        "        img1 = img1.rotate(rotate_degree, Image.BILINEAR)\n",
        "        img2 = img2.rotate(rotate_degree, Image.BILINEAR)\n",
        "        mask = mask.rotate(rotate_degree, Image.NEAREST)\n",
        "\n",
        "        return {'image': (img1, img2),\n",
        "                'label': mask}\n",
        "\n",
        "\n",
        "class RandomGaussianBlur(object):\n",
        "    def __call__(self, sample):\n",
        "        img1 = sample['image'][0]\n",
        "        img2 = sample['image'][1]\n",
        "        mask = sample['label']\n",
        "        if random.random() < 0.5:\n",
        "            img1 = img1.filter(ImageFilter.GaussianBlur(\n",
        "                radius=random.random()))\n",
        "            img2 = img2.filter(ImageFilter.GaussianBlur(\n",
        "                radius=random.random()))\n",
        "\n",
        "        return {'image': (img1, img2),\n",
        "                'label': mask}\n",
        "\n",
        "\n",
        "class RandomScaleCrop(object):\n",
        "    def __init__(self, base_size, crop_size, fill=0):\n",
        "        self.base_size = base_size\n",
        "        self.crop_size = crop_size\n",
        "        self.fill = fill\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        img = sample['image']\n",
        "        mask = sample['label']\n",
        "        # random scale (short edge)\n",
        "        short_size = random.randint(int(self.base_size * 0.5), int(self.base_size * 2.0))\n",
        "        w, h = img.size\n",
        "        if h > w:\n",
        "            ow = short_size\n",
        "            oh = int(1.0 * h * ow / w)\n",
        "        else:\n",
        "            oh = short_size\n",
        "            ow = int(1.0 * w * oh / h)\n",
        "        img = img.resize((ow, oh), Image.BILINEAR)\n",
        "        mask = mask.resize((ow, oh), Image.NEAREST)\n",
        "        # pad crop\n",
        "        if short_size < self.crop_size:\n",
        "            padh = self.crop_size - oh if oh < self.crop_size else 0\n",
        "            padw = self.crop_size - ow if ow < self.crop_size else 0\n",
        "            img = ImageOps.expand(img, border=(0, 0, padw, padh), fill=0)\n",
        "            mask = ImageOps.expand(mask, border=(0, 0, padw, padh), fill=self.fill)\n",
        "        # random crop crop_size\n",
        "        w, h = img.size\n",
        "        x1 = random.randint(0, w - self.crop_size)\n",
        "        y1 = random.randint(0, h - self.crop_size)\n",
        "        img = img.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))\n",
        "        mask = mask.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))\n",
        "\n",
        "        return {'image': img,\n",
        "                'label': mask}\n",
        "\n",
        "\n",
        "class FixScaleCrop(object):\n",
        "    def __init__(self, crop_size):\n",
        "        self.crop_size = crop_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        img = sample['image']\n",
        "        mask = sample['label']\n",
        "        w, h = img.size\n",
        "        if w > h:\n",
        "            oh = self.crop_size\n",
        "            ow = int(1.0 * w * oh / h)\n",
        "        else:\n",
        "            ow = self.crop_size\n",
        "            oh = int(1.0 * h * ow / w)\n",
        "        img = img.resize((ow, oh), Image.BILINEAR)\n",
        "        mask = mask.resize((ow, oh), Image.NEAREST)\n",
        "        # center crop\n",
        "        w, h = img.size\n",
        "        x1 = int(round((w - self.crop_size) / 2.))\n",
        "        y1 = int(round((h - self.crop_size) / 2.))\n",
        "        img = img.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))\n",
        "        mask = mask.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))\n",
        "\n",
        "        return {'image': img,\n",
        "                'label': mask}\n",
        "\n",
        "class FixedResize(object):\n",
        "    def __init__(self, size):\n",
        "        self.size = (size, size)  # size: (h, w)\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        img1 = sample['image'][0]\n",
        "        img2 = sample['image'][1]\n",
        "        mask = sample['label']\n",
        "\n",
        "        assert img1.size == mask.size and img2.size == mask.size\n",
        "\n",
        "        img1 = img1.resize(self.size, Image.BILINEAR)\n",
        "        img2 = img2.resize(self.size, Image.BILINEAR)\n",
        "        mask = mask.resize(self.size, Image.NEAREST)\n",
        "\n",
        "        return {'image': (img1, img2),\n",
        "                'label': mask}\n",
        "\n",
        "\n",
        "'''\n",
        "We don't use Normalize here, because it will bring negative effects.\n",
        "the mask of ground truth is converted to [0,1] in ToTensor() function.\n",
        "'''\n",
        "train_transforms = transforms.Compose([\n",
        "            RandomHorizontalFlip(),\n",
        "            RandomVerticalFlip(),\n",
        "            RandomFixRotate(),\n",
        "            # RandomScaleCrop(base_size=self.args.base_size, crop_size=self.args.crop_size),\n",
        "            # RandomGaussianBlur(),\n",
        "            # Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            ToTensor()])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "            # RandomHorizontalFlip(),\n",
        "            # RandomVerticalFlip(),\n",
        "            # RandomFixRotate(),\n",
        "            # RandomScaleCrop(base_size=self.args.base_size, crop_size=self.args.crop_size),\n",
        "            # RandomGaussianBlur(),\n",
        "            # Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            ToTensor()])"
      ],
      "metadata": {
        "id": "4Vmf4HaYEnE2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch.utils.data as data\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Load all training and validation data paths\n",
        "'''\n",
        "def full_path_loader(data_dir):\n",
        "    train_data = [i for i in os.listdir(data_dir + 'train/A/') if not\n",
        "    i.startswith('.')]\n",
        "    train_data.sort()\n",
        "\n",
        "    valid_data = [i for i in os.listdir(data_dir + 'val/A/') if not\n",
        "    i.startswith('.')]\n",
        "    valid_data.sort()\n",
        "\n",
        "    train_label_paths = []\n",
        "    val_label_paths = []\n",
        "    for img in train_data:\n",
        "        train_label_paths.append(data_dir + 'train/OUT/' + img)\n",
        "    for img in valid_data:\n",
        "        val_label_paths.append(data_dir + 'val/OUT/' + img)\n",
        "\n",
        "\n",
        "    train_data_path = []\n",
        "    val_data_path = []\n",
        "\n",
        "    for img in train_data:\n",
        "        train_data_path.append([data_dir + 'train/', img])\n",
        "    for img in valid_data:\n",
        "        val_data_path.append([data_dir + 'val/', img])\n",
        "\n",
        "    train_dataset = {}\n",
        "    val_dataset = {}\n",
        "    for cp in range(len(train_data)):\n",
        "        train_dataset[cp] = {'image': train_data_path[cp],\n",
        "                         'label': train_label_paths[cp]}\n",
        "    for cp in range(len(valid_data)):\n",
        "        val_dataset[cp] = {'image': val_data_path[cp],\n",
        "                         'label': val_label_paths[cp]}\n",
        "\n",
        "\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "'''\n",
        "Load all testing data paths\n",
        "'''\n",
        "def full_test_loader(data_dir):\n",
        "\n",
        "    test_data = [i for i in os.listdir(data_dir + 'test/A/') if not\n",
        "                    i.startswith('.')]\n",
        "    test_data.sort()\n",
        "\n",
        "    test_label_paths = []\n",
        "    for img in test_data:\n",
        "        test_label_paths.append(data_dir + 'test/OUT/' + img)\n",
        "\n",
        "    test_data_path = []\n",
        "    for img in test_data:\n",
        "        test_data_path.append([data_dir + 'test/', img])\n",
        "\n",
        "    test_dataset = {}\n",
        "    for cp in range(len(test_data)):\n",
        "        test_dataset[cp] = {'image': test_data_path[cp],\n",
        "                           'label': test_label_paths[cp]}\n",
        "\n",
        "    return test_dataset\n",
        "\n",
        "def cdd_loader(img_path, label_path, aug):\n",
        "    dir = img_path[0]\n",
        "    name = img_path[1]\n",
        "\n",
        "    img1 = Image.open(dir + 'A/' + name)\n",
        "    img2 = Image.open(dir + 'B/' + name)\n",
        "    label = Image.open(label_path)\n",
        "    sample = {'image': (img1, img2), 'label': label}\n",
        "\n",
        "    if aug:\n",
        "        sample = train_transforms(sample)\n",
        "    else:\n",
        "        sample = test_transforms(sample)\n",
        "\n",
        "    return sample['image'][0], sample['image'][1], sample['label']\n",
        "\n",
        "\n",
        "class CDDloader(data.Dataset):\n",
        "\n",
        "    def __init__(self, full_load, aug=False):\n",
        "\n",
        "        self.full_load = full_load\n",
        "        self.loader = cdd_loader\n",
        "        self.aug = aug\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        img_path, label_path = self.full_load[index]['image'], self.full_load[index]['label']\n",
        "\n",
        "        return self.loader(img_path,\n",
        "                           label_path,\n",
        "                           self.aug)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.full_load)\n"
      ],
      "metadata": {
        "id": "mSAQ67f24JcF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        if isinstance(alpha, (float, int)):\n",
        "            self.alpha = torch.Tensor([alpha, 1-alpha])\n",
        "        if isinstance(alpha, list):\n",
        "            self.alpha = torch.Tensor(alpha)\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        if input.dim() > 2:\n",
        "            # N,C,H,W => N,C,H*W\n",
        "            input = input.view(input.size(0), input.size(1), -1)\n",
        "\n",
        "            # N,C,H*W => N,H*W,C\n",
        "            input = input.transpose(1, 2)\n",
        "\n",
        "            # N,H*W,C => N*H*W,C\n",
        "            input = input.contiguous().view(-1, input.size(2))\n",
        "\n",
        "\n",
        "        target = target.view(-1, 1)\n",
        "        logpt = F.log_softmax(input)\n",
        "        logpt = logpt.gather(1, target)\n",
        "        logpt = logpt.view(-1)\n",
        "        pt = Variable(logpt.data.exp())\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            if self.alpha.type() != input.data.type():\n",
        "                self.alpha = self.alpha.type_as(input.data)\n",
        "            at = self.alpha.gather(0, target.data.view(-1))\n",
        "            logpt = logpt * Variable(at)\n",
        "\n",
        "        loss = -1 * (1-pt)**self.gamma * logpt\n",
        "\n",
        "        if self.size_average:\n",
        "            return loss.mean()\n",
        "        else:\n",
        "            return loss.sum()\n",
        "\n",
        "def dice_loss(logits, true, eps=1e-7):\n",
        "    \"\"\"Computes the Sørensen–Dice loss.\n",
        "    Note that PyTorch optimizers minimize a loss. In this\n",
        "    case, we would like to maximize the dice loss so we\n",
        "    return the negated dice loss.\n",
        "    Args:\n",
        "        true: a tensor of shape [B, 1, H, W].\n",
        "        logits: a tensor of shape [B, C, H, W]. Corresponds to\n",
        "            the raw output or logits of the model.\n",
        "        eps: added to the denominator for numerical stability.\n",
        "    Returns:\n",
        "        dice_loss: the Sørensen–Dice loss.\n",
        "    \"\"\"\n",
        "    num_classes = logits.shape[1]\n",
        "    if num_classes == 1:\n",
        "        true_1_hot = torch.eye(num_classes + 1)[true.squeeze(1)]\n",
        "        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n",
        "        true_1_hot_f = true_1_hot[:, 0:1, :, :]\n",
        "        true_1_hot_s = true_1_hot[:, 1:2, :, :]\n",
        "        true_1_hot = torch.cat([true_1_hot_s, true_1_hot_f], dim=1)\n",
        "        pos_prob = torch.sigmoid(logits)\n",
        "        neg_prob = 1 - pos_prob\n",
        "        probas = torch.cat([pos_prob, neg_prob], dim=1)\n",
        "    else:\n",
        "        true_1_hot = torch.eye(num_classes)[true.squeeze(1)]\n",
        "        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n",
        "        probas = F.softmax(logits, dim=1)\n",
        "    true_1_hot = true_1_hot.type(logits.type())\n",
        "    dims = (0,) + tuple(range(2, true.ndimension()))\n",
        "    intersection = torch.sum(probas * true_1_hot, dims)\n",
        "    cardinality = torch.sum(probas + true_1_hot, dims)\n",
        "    dice_loss = (2. * intersection / (cardinality + eps)).mean()\n",
        "    return (1 - dice_loss)\n",
        "\n",
        "\n",
        "def jaccard_loss(logits, true, eps=1e-7):\n",
        "    \"\"\"Computes the Jaccard loss, a.k.a the IoU loss.\n",
        "    Note that PyTorch optimizers minimize a loss. In this\n",
        "    case, we would like to maximize the jaccard loss so we\n",
        "    return the negated jaccard loss.\n",
        "    Args:\n",
        "        true: a tensor of shape [B, H, W] or [B, 1, H, W].\n",
        "        logits: a tensor of shape [B, C, H, W]. Corresponds to\n",
        "            the raw output or logits of the model.\n",
        "        eps: added to the denominator for numerical stability.\n",
        "    Returns:\n",
        "        jacc_loss: the Jaccard loss.\n",
        "    \"\"\"\n",
        "    num_classes = logits.shape[1]\n",
        "    if num_classes == 1:\n",
        "        true_1_hot = torch.eye(num_classes + 1)[true.squeeze(1)]\n",
        "        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n",
        "        true_1_hot_f = true_1_hot[:, 0:1, :, :]\n",
        "        true_1_hot_s = true_1_hot[:, 1:2, :, :]\n",
        "        true_1_hot = torch.cat([true_1_hot_s, true_1_hot_f], dim=1)\n",
        "        pos_prob = torch.sigmoid(logits)\n",
        "        neg_prob = 1 - pos_prob\n",
        "        probas = torch.cat([pos_prob, neg_prob], dim=1)\n",
        "    else:\n",
        "        true_1_hot = torch.eye(num_classes)[true.squeeze(1)]\n",
        "        true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n",
        "        probas = F.softmax(logits, dim=1)\n",
        "    true_1_hot = true_1_hot.type(logits.type())\n",
        "    dims = (0,) + tuple(range(2, true.ndimension()))\n",
        "    intersection = torch.sum(probas * true_1_hot, dims)\n",
        "    cardinality = torch.sum(probas + true_1_hot, dims)\n",
        "    union = cardinality - intersection\n",
        "    jacc_loss = (intersection / (union + eps)).mean()\n",
        "    return (1 - jacc_loss)\n",
        "\n",
        "\n",
        "class TverskyLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.5, beta=0.5, eps=1e-7, size_average=True):\n",
        "        super(TverskyLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.size_average = size_average\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, logits, true):\n",
        "        \"\"\"Computes the Tversky loss [1].\n",
        "        Args:\n",
        "            true: a tensor of shape [B, H, W] or [B, 1, H, W].\n",
        "            logits: a tensor of shape [B, C, H, W]. Corresponds to\n",
        "                the raw output or logits of the model.\n",
        "            alpha: controls the penalty for false positives.\n",
        "            beta: controls the penalty for false negatives.\n",
        "            eps: added to the denominator for numerical stability.\n",
        "        Returns:\n",
        "            tversky_loss: the Tversky loss.\n",
        "        Notes:\n",
        "            alpha = beta = 0.5 => dice coeff\n",
        "            alpha = beta = 1 => tanimoto coeff\n",
        "            alpha + beta = 1 => F beta coeff\n",
        "        References:\n",
        "            [1]: https://arxiv.org/abs/1706.05721\n",
        "        \"\"\"\n",
        "        num_classes = logits.shape[1]\n",
        "        if num_classes == 1:\n",
        "            true_1_hot = torch.eye(num_classes + 1)[true.squeeze(1)]\n",
        "            true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n",
        "            true_1_hot_f = true_1_hot[:, 0:1, :, :]\n",
        "            true_1_hot_s = true_1_hot[:, 1:2, :, :]\n",
        "            true_1_hot = torch.cat([true_1_hot_s, true_1_hot_f], dim=1)\n",
        "            pos_prob = torch.sigmoid(logits)\n",
        "            neg_prob = 1 - pos_prob\n",
        "            probas = torch.cat([pos_prob, neg_prob], dim=1)\n",
        "        else:\n",
        "            true_1_hot = torch.eye(num_classes)[true.squeeze(1)]\n",
        "            true_1_hot = true_1_hot.permute(0, 3, 1, 2).float()\n",
        "            probas = F.softmax(logits, dim=1)\n",
        "\n",
        "        true_1_hot = true_1_hot.type(logits.type())\n",
        "        dims = (0,) + tuple(range(2, true.ndimension()))\n",
        "        intersection = torch.sum(probas * true_1_hot, dims)\n",
        "        fps = torch.sum(probas * (1 - true_1_hot), dims)\n",
        "        fns = torch.sum((1 - probas) * true_1_hot, dims)\n",
        "        num = intersection\n",
        "        denom = intersection + (self.alpha * fps) + (self.beta * fns)\n",
        "        tversky_loss = (num / (denom + self.eps)).mean()\n",
        "        return (1 - tversky_loss)"
      ],
      "metadata": {
        "id": "PLVNL5qH4JZf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse as ag\n",
        "import json\n",
        "\n",
        "def get_parser_with_args(metadata_json= \"metadata.json\"):\n",
        "    parser = ag.ArgumentParser(description='Training change detection network')\n",
        "\n",
        "    with open(metadata_json, 'r') as fin:\n",
        "        metadata = json.load(fin)\n",
        "        parser.set_defaults(**metadata)\n",
        "        return parser, metadata\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "Y8QqFR_-38y9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser, metadata = get_parser_with_args(\"metadata.json\")\n",
        "import sys \n",
        "argv =sys.argv \n",
        "sys.argv = sys.argv[:1]\n",
        "opt = parser.parse_args()\n",
        "sys.argv = argv\n",
        "\n",
        "def hybrid_loss(predictions, target):\n",
        "    \"\"\"Calculating the loss\"\"\"\n",
        "    loss = 0\n",
        "\n",
        "    # gamma=0, alpha=None --> CE\n",
        "    focal = FocalLoss(gamma=0, alpha=None)\n",
        "\n",
        "    for prediction in predictions:\n",
        "\n",
        "        bce = focal(prediction, target)\n",
        "        dice = dice_loss(prediction, target)\n",
        "        loss += bce + dice\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Fkyh1mUE384U"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class conv_block_nested(nn.Module):\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(conv_block_nested, self).__init__()\n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=True)\n",
        "        self.bn1 = nn.BatchNorm2d(mid_ch)\n",
        "        self.conv2 = nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=True)\n",
        "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        identity = x\n",
        "        x = self.bn1(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        output = self.activation(x + identity)\n",
        "        return output\n",
        "\n",
        "\n",
        "class up(nn.Module):\n",
        "    def __init__(self, in_ch, bilinear=False):\n",
        "        super(up, self).__init__()\n",
        "\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2,\n",
        "                                  mode='bilinear',\n",
        "                                  align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_ch, in_ch, 2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_channels, ratio = 16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.fc1 = nn.Conv2d(in_channels,in_channels//ratio,1,bias=False)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Conv2d(in_channels//ratio, in_channels,1,bias=False)\n",
        "        self.sigmod = nn.Sigmoid()\n",
        "    def forward(self,x):\n",
        "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
        "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmod(out)\n",
        "\n",
        "\n",
        "\n",
        "class SNUNet_ECAM(nn.Module):\n",
        "    # SNUNet-CD with ECAM\n",
        "    def __init__(self, in_ch=3, out_ch=2):\n",
        "        super(SNUNet_ECAM, self).__init__()\n",
        "        torch.nn.Module.dump_patches = True\n",
        "        n1 = 32     # the initial number of channels of feature map\n",
        "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv0_0 = conv_block_nested(in_ch, filters[0], filters[0])\n",
        "        self.conv1_0 = conv_block_nested(filters[0], filters[1], filters[1])\n",
        "        self.Up1_0 = up(filters[1])\n",
        "        self.conv2_0 = conv_block_nested(filters[1], filters[2], filters[2])\n",
        "        self.Up2_0 = up(filters[2])\n",
        "        self.conv3_0 = conv_block_nested(filters[2], filters[3], filters[3])\n",
        "        self.Up3_0 = up(filters[3])\n",
        "        self.conv4_0 = conv_block_nested(filters[3], filters[4], filters[4])\n",
        "        self.Up4_0 = up(filters[4])\n",
        "\n",
        "        self.conv0_1 = conv_block_nested(filters[0] * 2 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_1 = conv_block_nested(filters[1] * 2 + filters[2], filters[1], filters[1])\n",
        "        self.Up1_1 = up(filters[1])\n",
        "        self.conv2_1 = conv_block_nested(filters[2] * 2 + filters[3], filters[2], filters[2])\n",
        "        self.Up2_1 = up(filters[2])\n",
        "        self.conv3_1 = conv_block_nested(filters[3] * 2 + filters[4], filters[3], filters[3])\n",
        "        self.Up3_1 = up(filters[3])\n",
        "\n",
        "        self.conv0_2 = conv_block_nested(filters[0] * 3 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_2 = conv_block_nested(filters[1] * 3 + filters[2], filters[1], filters[1])\n",
        "        self.Up1_2 = up(filters[1])\n",
        "        self.conv2_2 = conv_block_nested(filters[2] * 3 + filters[3], filters[2], filters[2])\n",
        "        self.Up2_2 = up(filters[2])\n",
        "\n",
        "        self.conv0_3 = conv_block_nested(filters[0] * 4 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_3 = conv_block_nested(filters[1] * 4 + filters[2], filters[1], filters[1])\n",
        "        self.Up1_3 = up(filters[1])\n",
        "\n",
        "        self.conv0_4 = conv_block_nested(filters[0] * 5 + filters[1], filters[0], filters[0])\n",
        "\n",
        "        self.ca = ChannelAttention(filters[0] * 4, ratio=16)\n",
        "        self.ca1 = ChannelAttention(filters[0], ratio=16 // 4)\n",
        "\n",
        "        self.conv_final = nn.Conv2d(filters[0] * 4, out_ch, kernel_size=1)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "    def forward(self, xA, xB):\n",
        "        '''xA'''\n",
        "        x0_0A = self.conv0_0(xA)\n",
        "        x1_0A = self.conv1_0(self.pool(x0_0A))\n",
        "        x2_0A = self.conv2_0(self.pool(x1_0A))\n",
        "        x3_0A = self.conv3_0(self.pool(x2_0A))\n",
        "        # x4_0A = self.conv4_0(self.pool(x3_0A))\n",
        "        '''xB'''\n",
        "        x0_0B = self.conv0_0(xB)\n",
        "        x1_0B = self.conv1_0(self.pool(x0_0B))\n",
        "        x2_0B = self.conv2_0(self.pool(x1_0B))\n",
        "        x3_0B = self.conv3_0(self.pool(x2_0B))\n",
        "        x4_0B = self.conv4_0(self.pool(x3_0B))\n",
        "\n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0A, x0_0B, self.Up1_0(x1_0B)], 1))\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0A, x1_0B, self.Up2_0(x2_0B)], 1))\n",
        "        x0_2 = self.conv0_2(torch.cat([x0_0A, x0_0B, x0_1, self.Up1_1(x1_1)], 1))\n",
        "\n",
        "\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0A, x2_0B, self.Up3_0(x3_0B)], 1))\n",
        "        x1_2 = self.conv1_2(torch.cat([x1_0A, x1_0B, x1_1, self.Up2_1(x2_1)], 1))\n",
        "        x0_3 = self.conv0_3(torch.cat([x0_0A, x0_0B, x0_1, x0_2, self.Up1_2(x1_2)], 1))\n",
        "\n",
        "        x3_1 = self.conv3_1(torch.cat([x3_0A, x3_0B, self.Up4_0(x4_0B)], 1))\n",
        "        x2_2 = self.conv2_2(torch.cat([x2_0A, x2_0B, x2_1, self.Up3_1(x3_1)], 1))\n",
        "        x1_3 = self.conv1_3(torch.cat([x1_0A, x1_0B, x1_1, x1_2, self.Up2_2(x2_2)], 1))\n",
        "        x0_4 = self.conv0_4(torch.cat([x0_0A, x0_0B, x0_1, x0_2, x0_3, self.Up1_3(x1_3)], 1))\n",
        "\n",
        "        out = torch.cat([x0_1, x0_2, x0_3, x0_4], 1)\n",
        "\n",
        "        intra = torch.sum(torch.stack((x0_1, x0_2, x0_3, x0_4)), dim=0)\n",
        "        ca1 = self.ca1(intra)\n",
        "        out = self.ca(out) * (out + ca1.repeat(1, 4, 1, 1))\n",
        "        out = self.conv_final(out)\n",
        "\n",
        "        return (out, )\n",
        "\n",
        "\n",
        "class Siam_NestedUNet_Conc(nn.Module):\n",
        "    # SNUNet-CD without Attention\n",
        "    def __init__(self, in_ch=3, out_ch=2):\n",
        "        super(Siam_NestedUNet_Conc, self).__init__()\n",
        "        torch.nn.Module.dump_patches = True\n",
        "        n1 = 32     # the initial number of channels of feature map\n",
        "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv0_0 = conv_block_nested(in_ch, filters[0], filters[0])\n",
        "        self.conv1_0 = conv_block_nested(filters[0], filters[1], filters[1])\n",
        "        self.Up1_0 = up(filters[1])\n",
        "        self.conv2_0 = conv_block_nested(filters[1], filters[2], filters[2])\n",
        "        self.Up2_0 = up(filters[2])\n",
        "        self.conv3_0 = conv_block_nested(filters[2], filters[3], filters[3])\n",
        "        self.Up3_0 = up(filters[3])\n",
        "        self.conv4_0 = conv_block_nested(filters[3], filters[4], filters[4])\n",
        "        self.Up4_0 = up(filters[4])\n",
        "\n",
        "        self.conv0_1 = conv_block_nested(filters[0] * 2 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_1 = conv_block_nested(filters[1] * 2 + filters[2], filters[1], filters[1])\n",
        "        self.Up1_1 = up(filters[1])\n",
        "        self.conv2_1 = conv_block_nested(filters[2] * 2 + filters[3], filters[2], filters[2])\n",
        "        self.Up2_1 = up(filters[2])\n",
        "        self.conv3_1 = conv_block_nested(filters[3] * 2 + filters[4], filters[3], filters[3])\n",
        "        self.Up3_1 = up(filters[3])\n",
        "\n",
        "        self.conv0_2 = conv_block_nested(filters[0] * 3 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_2 = conv_block_nested(filters[1] * 3 + filters[2], filters[1], filters[1])\n",
        "        self.Up1_2 = up(filters[1])\n",
        "        self.conv2_2 = conv_block_nested(filters[2] * 3 + filters[3], filters[2], filters[2])\n",
        "        self.Up2_2 = up(filters[2])\n",
        "\n",
        "        self.conv0_3 = conv_block_nested(filters[0] * 4 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_3 = conv_block_nested(filters[1] * 4 + filters[2], filters[1], filters[1])\n",
        "        self.Up1_3 = up(filters[1])\n",
        "\n",
        "        self.conv0_4 = conv_block_nested(filters[0] * 5 + filters[1], filters[0], filters[0])\n",
        "\n",
        "        self.final1 = nn.Conv2d(filters[0], out_ch, kernel_size=1)\n",
        "        self.final2 = nn.Conv2d(filters[0], out_ch, kernel_size=1)\n",
        "        self.final3 = nn.Conv2d(filters[0], out_ch, kernel_size=1)\n",
        "        self.final4 = nn.Conv2d(filters[0], out_ch, kernel_size=1)\n",
        "        self.conv_final = nn.Conv2d(out_ch * 4, out_ch, kernel_size=1)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "    def forward(self, xA, xB):\n",
        "        '''xA'''\n",
        "        x0_0A = self.conv0_0(xA)\n",
        "        x1_0A = self.conv1_0(self.pool(x0_0A))\n",
        "        x2_0A = self.conv2_0(self.pool(x1_0A))\n",
        "        x3_0A = self.conv3_0(self.pool(x2_0A))\n",
        "        # x4_0A = self.conv4_0(self.pool(x3_0A))\n",
        "        '''xB'''\n",
        "        x0_0B = self.conv0_0(xB)\n",
        "        x1_0B = self.conv1_0(self.pool(x0_0B))\n",
        "        x2_0B = self.conv2_0(self.pool(x1_0B))\n",
        "        x3_0B = self.conv3_0(self.pool(x2_0B))\n",
        "        x4_0B = self.conv4_0(self.pool(x3_0B))\n",
        "\n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0A, x0_0B, self.Up1_0(x1_0B)], 1))\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0A, x1_0B, self.Up2_0(x2_0B)], 1))\n",
        "        x0_2 = self.conv0_2(torch.cat([x0_0A, x0_0B, x0_1, self.Up1_1(x1_1)], 1))\n",
        "\n",
        "\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0A, x2_0B, self.Up3_0(x3_0B)], 1))\n",
        "        x1_2 = self.conv1_2(torch.cat([x1_0A, x1_0B, x1_1, self.Up2_1(x2_1)], 1))\n",
        "        x0_3 = self.conv0_3(torch.cat([x0_0A, x0_0B, x0_1, x0_2, self.Up1_2(x1_2)], 1))\n",
        "\n",
        "        x3_1 = self.conv3_1(torch.cat([x3_0A, x3_0B, self.Up4_0(x4_0B)], 1))\n",
        "        x2_2 = self.conv2_2(torch.cat([x2_0A, x2_0B, x2_1, self.Up3_1(x3_1)], 1))\n",
        "        x1_3 = self.conv1_3(torch.cat([x1_0A, x1_0B, x1_1, x1_2, self.Up2_2(x2_2)], 1))\n",
        "        x0_4 = self.conv0_4(torch.cat([x0_0A, x0_0B, x0_1, x0_2, x0_3, self.Up1_3(x1_3)], 1))\n",
        "\n",
        "\n",
        "        output1 = self.final1(x0_1)\n",
        "        output2 = self.final2(x0_2)\n",
        "        output3 = self.final3(x0_3)\n",
        "        output4 = self.final4(x0_4)\n",
        "        output = self.conv_final(torch.cat([output1, output2, output3, output4], 1))\n",
        "        return (output1, output2, output3, output4, output)"
      ],
      "metadata": {
        "id": "_ZmIZBj8387T"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rodrigo Caye Daudt\n",
        "# https://rcdaudt.github.io/\n",
        "# Daudt, R. C., Le Saux, B., & Boulch, A. \"Fully convolutional siamese networks for change detection\". In 2018 25th IEEE International Conference on Image Processing (ICIP) (pp. 4063-4067). IEEE.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.padding import ReplicationPad2d\n",
        "\n",
        "class SiamUnet_diff(nn.Module):\n",
        "    \"\"\"SiamUnet_diff segmentation network.\"\"\"\n",
        "\n",
        "    def __init__(self, input_nbr, label_nbr):\n",
        "        super(SiamUnet_diff, self).__init__()\n",
        "\n",
        "        n1 = 16\n",
        "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
        "        self.input_nbr = input_nbr\n",
        "\n",
        "        self.conv11 = nn.Conv2d(input_nbr, filters[0], kernel_size=3, padding=1)\n",
        "        self.bn11 = nn.BatchNorm2d(filters[0])\n",
        "        self.do11 = nn.Dropout2d(p=0.2)\n",
        "        self.conv12 = nn.Conv2d(filters[0], filters[0], kernel_size=3, padding=1)\n",
        "        self.bn12 = nn.BatchNorm2d(filters[0])\n",
        "        self.do12 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv21 = nn.Conv2d(filters[0], filters[1], kernel_size=3, padding=1)\n",
        "        self.bn21 = nn.BatchNorm2d(filters[1])\n",
        "        self.do21 = nn.Dropout2d(p=0.2)\n",
        "        self.conv22 = nn.Conv2d(filters[1], filters[1], kernel_size=3, padding=1)\n",
        "        self.bn22 = nn.BatchNorm2d(filters[1])\n",
        "        self.do22 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv31 = nn.Conv2d(filters[1], filters[2], kernel_size=3, padding=1)\n",
        "        self.bn31 = nn.BatchNorm2d(filters[2])\n",
        "        self.do31 = nn.Dropout2d(p=0.2)\n",
        "        self.conv32 = nn.Conv2d(filters[2], filters[2], kernel_size=3, padding=1)\n",
        "        self.bn32 = nn.BatchNorm2d(filters[2])\n",
        "        self.do32 = nn.Dropout2d(p=0.2)\n",
        "        self.conv33 = nn.Conv2d(filters[2], filters[2], kernel_size=3, padding=1)\n",
        "        self.bn33 = nn.BatchNorm2d(filters[2])\n",
        "        self.do33 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.conv41 = nn.Conv2d(filters[2], filters[3], kernel_size=3, padding=1)\n",
        "        self.bn41 = nn.BatchNorm2d(filters[3])\n",
        "        self.do41 = nn.Dropout2d(p=0.2)\n",
        "        self.conv42 = nn.Conv2d(filters[3], filters[3], kernel_size=3, padding=1)\n",
        "        self.bn42 = nn.BatchNorm2d(filters[3])\n",
        "        self.do42 = nn.Dropout2d(p=0.2)\n",
        "        self.conv43 = nn.Conv2d(filters[3], filters[3], kernel_size=3, padding=1)\n",
        "        self.bn43 = nn.BatchNorm2d(filters[3])\n",
        "        self.do43 = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(filters[3], filters[3], kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv43d = nn.ConvTranspose2d(filters[4], filters[3], kernel_size=3, padding=1)\n",
        "        self.bn43d = nn.BatchNorm2d(filters[3])\n",
        "        self.do43d = nn.Dropout2d(p=0.2)\n",
        "        self.conv42d = nn.ConvTranspose2d(filters[3], filters[3], kernel_size=3, padding=1)\n",
        "        self.bn42d = nn.BatchNorm2d(filters[3])\n",
        "        self.do42d = nn.Dropout2d(p=0.2)\n",
        "        self.conv41d = nn.ConvTranspose2d(filters[3], filters[2], kernel_size=3, padding=1)\n",
        "        self.bn41d = nn.BatchNorm2d(filters[2])\n",
        "        self.do41d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(filters[2], filters[2], kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv33d = nn.ConvTranspose2d(filters[3], filters[2], kernel_size=3, padding=1)\n",
        "        self.bn33d = nn.BatchNorm2d(filters[2])\n",
        "        self.do33d = nn.Dropout2d(p=0.2)\n",
        "        self.conv32d = nn.ConvTranspose2d(filters[2], filters[2], kernel_size=3, padding=1)\n",
        "        self.bn32d = nn.BatchNorm2d(filters[2])\n",
        "        self.do32d = nn.Dropout2d(p=0.2)\n",
        "        self.conv31d = nn.ConvTranspose2d(filters[2], filters[1], kernel_size=3, padding=1)\n",
        "        self.bn31d = nn.BatchNorm2d(filters[1])\n",
        "        self.do31d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(filters[1], filters[1], kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv22d = nn.ConvTranspose2d(filters[2], filters[1], kernel_size=3, padding=1)\n",
        "        self.bn22d = nn.BatchNorm2d(filters[1])\n",
        "        self.do22d = nn.Dropout2d(p=0.2)\n",
        "        self.conv21d = nn.ConvTranspose2d(filters[1], filters[0], kernel_size=3, padding=1)\n",
        "        self.bn21d = nn.BatchNorm2d(filters[0])\n",
        "        self.do21d = nn.Dropout2d(p=0.2)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(filters[0], filters[0], kernel_size=3, padding=1, stride=2, output_padding=1)\n",
        "\n",
        "        self.conv12d = nn.ConvTranspose2d(filters[1], filters[0], kernel_size=3, padding=1)\n",
        "        self.bn12d = nn.BatchNorm2d(filters[0])\n",
        "        self.do12d = nn.Dropout2d(p=0.2)\n",
        "        self.conv11d = nn.ConvTranspose2d(filters[0], label_nbr, kernel_size=3, padding=1)\n",
        "\n",
        "        self.sm = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "\n",
        "\n",
        "        \"\"\"Forward method.\"\"\"\n",
        "        # Stage 1\n",
        "        x11 = self.do11(F.relu(self.bn11(self.conv11(x1))))\n",
        "        x12_1 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
        "        x1p = F.max_pool2d(x12_1, kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "        # Stage 2\n",
        "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
        "        x22_1 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
        "        x2p = F.max_pool2d(x22_1, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 3\n",
        "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
        "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
        "        x33_1 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
        "        x3p = F.max_pool2d(x33_1, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 4\n",
        "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
        "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
        "        x43_1 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
        "        x4p = F.max_pool2d(x43_1, kernel_size=2, stride=2)\n",
        "\n",
        "        ####################################################\n",
        "        # Stage 1\n",
        "        x11 = self.do11(F.relu(self.bn11(self.conv11(x2))))\n",
        "        x12_2 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
        "        x1p = F.max_pool2d(x12_2, kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "        # Stage 2\n",
        "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
        "        x22_2 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
        "        x2p = F.max_pool2d(x22_2, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 3\n",
        "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
        "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
        "        x33_2 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
        "        x3p = F.max_pool2d(x33_2, kernel_size=2, stride=2)\n",
        "\n",
        "        # Stage 4\n",
        "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
        "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
        "        x43_2 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
        "        x4p = F.max_pool2d(x43_2, kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "\n",
        "        # Stage 4d\n",
        "        x4d = self.upconv4(x4p)\n",
        "        pad4 = ReplicationPad2d((0, x43_1.size(3) - x4d.size(3), 0, x43_1.size(2) - x4d.size(2)))\n",
        "        x4d = torch.cat((pad4(x4d), torch.abs(x43_1 - x43_2)), 1)\n",
        "        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n",
        "        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n",
        "        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n",
        "\n",
        "        # Stage 3d\n",
        "        x3d = self.upconv3(x41d)\n",
        "        pad3 = ReplicationPad2d((0, x33_1.size(3) - x3d.size(3), 0, x33_1.size(2) - x3d.size(2)))\n",
        "        x3d = torch.cat((pad3(x3d), torch.abs(x33_1 - x33_2)), 1)\n",
        "        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n",
        "        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n",
        "        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n",
        "\n",
        "        # Stage 2d\n",
        "        x2d = self.upconv2(x31d)\n",
        "        pad2 = ReplicationPad2d((0, x22_1.size(3) - x2d.size(3), 0, x22_1.size(2) - x2d.size(2)))\n",
        "        x2d = torch.cat((pad2(x2d), torch.abs(x22_1 - x22_2)), 1)\n",
        "        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n",
        "        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n",
        "\n",
        "        # Stage 1d\n",
        "        x1d = self.upconv1(x21d)\n",
        "        pad1 = ReplicationPad2d((0, x12_1.size(3) - x1d.size(3), 0, x12_1.size(2) - x1d.size(2)))\n",
        "        x1d = torch.cat((pad1(x1d), torch.abs(x12_1 - x12_2)), 1)\n",
        "        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n",
        "        x11d = self.conv11d(x12d)\n",
        "\n",
        "        return (x11d, )\n",
        "        # return self.sm(x11d)"
      ],
      "metadata": {
        "id": "ayzCWQDd38-O"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "def initialize_metrics():\n",
        "    \"\"\"Generates a dictionary of metrics with metrics as keys\n",
        "       and empty lists as values\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        a dictionary of metrics\n",
        "    \"\"\"\n",
        "    metrics = {\n",
        "        'cd_losses': [],\n",
        "        'cd_corrects': [],\n",
        "        'cd_precisions': [],\n",
        "        'cd_recalls': [],\n",
        "        'cd_f1scores': [],\n",
        "        'learning_rate': [],\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def get_mean_metrics(metric_dict):\n",
        "    \"\"\"takes a dictionary of lists for metrics and returns dict of mean values\n",
        "    Parameters\n",
        "    ----------\n",
        "    metric_dict : dict\n",
        "        A dictionary of metrics\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        dict of floats that reflect mean metric value\n",
        "    \"\"\"\n",
        "    return {k: np.mean(v) for k, v in metric_dict.items()}\n",
        "\n",
        "\n",
        "def set_metrics(metric_dict, cd_loss, cd_corrects, cd_report, lr):\n",
        "    \"\"\"Updates metric dict with batch metrics\n",
        "    Parameters\n",
        "    ----------\n",
        "    metric_dict : dict\n",
        "        dict of metrics\n",
        "    cd_loss : dict(?)\n",
        "        loss value\n",
        "    cd_corrects : dict(?)\n",
        "        number of correct results (to generate accuracy\n",
        "    cd_report : list\n",
        "        precision, recall, f1 values\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        dict of  updated metrics\n",
        "    \"\"\"\n",
        "    metric_dict['cd_losses'].append(cd_loss.item())\n",
        "    metric_dict['cd_corrects'].append(cd_corrects.item())\n",
        "    metric_dict['cd_precisions'].append(cd_report[0])\n",
        "    metric_dict['cd_recalls'].append(cd_report[1])\n",
        "    metric_dict['cd_f1scores'].append(cd_report[2])\n",
        "    metric_dict['learning_rate'].append(lr)\n",
        "\n",
        "    return metric_dict\n",
        "\n",
        "def set_test_metrics(metric_dict, cd_corrects, cd_report):\n",
        "\n",
        "    metric_dict['cd_corrects'].append(cd_corrects.item())\n",
        "    metric_dict['cd_precisions'].append(cd_report[0])\n",
        "    metric_dict['cd_recalls'].append(cd_report[1])\n",
        "    metric_dict['cd_f1scores'].append(cd_report[2])\n",
        "\n",
        "    return metric_dict\n",
        "\n",
        "\n",
        "def get_loaders(opt):\n",
        "\n",
        "\n",
        "    logging.info('STARTING Dataset Creation')\n",
        "\n",
        "    train_full_load, val_full_load = full_path_loader(opt.dataset_dir)\n",
        "\n",
        "\n",
        "    train_dataset = CDDloader(train_full_load, aug=opt.augmentation)\n",
        "    val_dataset = CDDloader(val_full_load, aug=False)\n",
        "\n",
        "    logging.info('STARTING Dataloading')\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                               batch_size=opt.batch_size,\n",
        "                                               shuffle=True,\n",
        "                                               num_workers=opt.num_workers)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                             batch_size=opt.batch_size,\n",
        "                                             shuffle=False,\n",
        "                                             num_workers=opt.num_workers)\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def get_test_loaders(opt, batch_size=None):\n",
        "\n",
        "    if not batch_size:\n",
        "        batch_size = opt.batch_size\n",
        "\n",
        "    logging.info('STARTING Dataset Creation')\n",
        "\n",
        "    test_full_load = full_test_loader(opt.dataset_dir)\n",
        "\n",
        "    test_dataset = CDDloader(test_full_load, aug=False)\n",
        "\n",
        "    logging.info('STARTING Dataloading')\n",
        "\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                             batch_size=batch_size,\n",
        "                                             shuffle=False,\n",
        "                                             num_workers=opt.num_workers)\n",
        "    return test_loader\n",
        "\n",
        "\n",
        "def get_criterion(opt):\n",
        "    \"\"\"get the user selected loss function\n",
        "    Parameters\n",
        "    ----------\n",
        "    opt : dict\n",
        "        Dictionary of options/flags\n",
        "    Returns\n",
        "    -------\n",
        "    method\n",
        "        loss function\n",
        "    \"\"\"\n",
        "    if opt.loss_function == 'hybrid':\n",
        "        criterion = hybrid_loss\n",
        "    if opt.loss_function == 'bce':\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "    if opt.loss_function == 'dice':\n",
        "        criterion = dice_loss\n",
        "    if opt.loss_function == 'jaccard':\n",
        "        criterion = jaccard_loss\n",
        "\n",
        "    return criterion\n",
        "\n",
        "\n",
        "def load_model(opt, device):\n",
        "    \"\"\"Load the model\n",
        "    Parameters\n",
        "    ----------\n",
        "    opt : dict\n",
        "        User specified flags/options\n",
        "    device : string\n",
        "        device on which to train model\n",
        "    \"\"\"\n",
        "    # device_ids = list(range(opt.num_gpus))\n",
        "    model = SNUNet_ECAM(opt.num_channel, 2).to(device)\n",
        "    # model = nn.DataParallel(model, device_ids=device_ids)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "rPOm9pdc39BU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "if not os.path.exists('./output_img'):\n",
        "    os.mkdir('./output_img')\n",
        "\n",
        "parser, metadata = get_parser_with_args()\n",
        "\n",
        "argv =sys.argv \n",
        "sys.argv = sys.argv[:1]\n",
        "opt = parser.parse_args()\n",
        "sys.argv = argv\n",
        "\n",
        "dev = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "test_loader = get_test_loaders(opt, batch_size=1)\n",
        "\n",
        "path = '/content/drive/MyDrive/mouhcine workspace/weights/snunet-32.pt'   # the path of the model\n",
        "model = torch.load(path)\n",
        "\n",
        "model.eval()\n",
        "index_img = 0\n",
        "test_metrics = initialize_metrics()\n",
        "with torch.no_grad():\n",
        "    tbar = tqdm(test_loader)\n",
        "    for batch_img1, batch_img2, labels in tbar:\n",
        "\n",
        "        batch_img1 = batch_img1.float().to(dev)\n",
        "        batch_img2 = batch_img2.float().to(dev)\n",
        "        labels = labels.long().to(dev)\n",
        "\n",
        "        cd_preds = model(batch_img1, batch_img2)\n",
        "\n",
        "        cd_preds = cd_preds[-1]\n",
        "        _, cd_preds = torch.max(cd_preds, 1)\n",
        "        cd_preds = cd_preds.data.cpu().numpy()\n",
        "        cd_preds = cd_preds.squeeze() * 255\n",
        "\n",
        "        file_path = './output_img/' + str(index_img).zfill(5)\n",
        "        cv2.imwrite(file_path + '.png', cd_preds)\n",
        "\n",
        "        index_img += 1"
      ],
      "metadata": {
        "id": "iVu2BFyj39D5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "db12ebbc-2228-4610-d15c-2d6f12684ece"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-af3300f7c53c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/mouhcine workspace/weights/snunet-32.pt'\u001b[0m   \u001b[0;31m# the path of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-0985cd414e00>\u001b[0m in \u001b[0;36mget_test_loaders\u001b[0;34m(opt, batch_size)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'STARTING Dataset Creation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mtest_full_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_test_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCDDloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_full_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-84eeabf567db>\u001b[0m in \u001b[0;36mfull_test_loader\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfull_test_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/mouhcine workspace\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     test_data = [i for i in os.listdir(data_dir + 'test/A/') if not\n\u001b[0m\u001b[1;32m     54\u001b[0m                     i.startswith('.')]\n\u001b[1;32m     55\u001b[0m     \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../CDData/subset/test/A/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CkyKZDdA39Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sN_ibXYq39Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A3Gv6-mF39MQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}